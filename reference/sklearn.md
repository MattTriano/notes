# sklearn

## General basic pattern (single holdout set)

1. preprocess data
2. import model class
3. create instance of a model with the desired parameters (better values can be found through hyperparameter tuning)
4. split data into train and test sets
5. fit model to training data
6. evaluate performance of fitted model on test data

The above evaluation method only uses a single train/test split, so it's possible that the model is being trained on a non-representative sampling of the data (risk increases as dataset size decreases). A more reliable evaluation method involves n-fold cross validation, which produces `n` different train-test splits of the data and uses each to train their own model instance and evaluate the performance of each of those trained model instances on the corresponding test data set. This produces one evalutation metric value for each train/test split and averaging them can produce a more representative measure of the model's performance.

When the achieved performance level meets your requirements, you can use the model to make predictions with new data.

# Model Classes

## Linear Regression

Linear regression models fit data to a linear equation and find the parameters that minimize the combined difference between observed and predicted values.

Linear equations have the form:

$$y = mx + b$$

where 

* $y$ is the outcome or target feature,
* $x$ is the input feature,
* $m$ is the slope of the line, and
* $b$ is the $y$-intercept (value of the equation when $x = 0$)

With real data (describing non-tautalogical relationships), data points can (and the majority do) deviate from the predictions generated by a linear model. This deviation is represented by the **error function** (also known as the **cost function** or **loss function**) and is represented by epsilon ($\epsilon$) in the equation below.

$$y = mx + b + \epsilon$$

* Multiple regression

$$y = m_1x_1 + m_2x_2 + \dots + m_nx_n + b $$

### Ordinary Least Squares (OLS) regression

In OLS, the standard error function is the **Residual Sum of Squares** (RSS)

$$ RSS = \Sum^n_{i=1}(y_i - \hat{y}_i)^2$$


```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)
```

### Evaluation

* Producing R-squared and root-mean-squared-error metrics

    ```python
    from sklearn.metrics import mean_squared_error

    r_squared = reg.score(X_test, y_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    ```

### Regularized Regression (Lasso and Ridge)

Regularization


# Usage Notes

Scikit-learn models require numeric dtypes and refuse to handle features with null values.

